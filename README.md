# ğŸ“ LifeRecordReview - AI ê¸°ë°˜ ìƒê¸°ë¶€ íŠ¹ê¸°ì‚¬í•­ ë¬¸ì¥ í‰ê°€ ì‹œìŠ¤í…œ

ê³ ë“±í•™êµ ìƒê¸°ë¶€ íŠ¹ê¸°ì‚¬í•­ ì‘ì„±ì„ ë„ì™€ì£¼ëŠ” AI ê¸°ë°˜ ê²€í†  ë„êµ¬ì…ë‹ˆë‹¤. ì‘ì„±í•œ íŠ¹ê¸°ì‚¬í•­ì„ ì…ë ¥í•˜ë©´ AIê°€ ë¬¸ì¥ì„ ë¶„ì„í•˜ê³  ê°œì„ ì ì„ ì œì•ˆí•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨
- [í”„ë¡œì íŠ¸ ì†Œê°œ](#í”„ë¡œì íŠ¸-ì†Œê°œ)
- [ê¸°ìˆ  ìŠ¤íƒ & ì•„í‚¤í…ì²˜](#ê¸°ìˆ -ìŠ¤íƒ--ì•„í‚¤í…ì²˜)
- [ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­](#ì‹œìŠ¤í…œ-ìš”êµ¬ì‚¬í•­)
- [ì„¤ì¹˜ ë° ì‹¤í–‰ ê°€ì´ë“œ](#ì„¤ì¹˜-ë°-ì‹¤í–‰-ê°€ì´ë“œ)
- [í™˜ê²½ ì„¤ì •](#í™˜ê²½-ì„¤ì •)
- [ì„œë¹„ìŠ¤ ë™ì‘ íë¦„](#ì„œë¹„ìŠ¤-ë™ì‘-íë¦„)
- [API ëª…ì„¸](#api-ëª…ì„¸)
- [ë¬¸ì œ í•´ê²°](#ë¬¸ì œ-í•´ê²°)
- [ë°°í¬ ê°€ì´ë“œ](#ë°°í¬-ê°€ì´ë“œ)

---

## ğŸš€ í”„ë¡œì íŠ¸ ì†Œê°œ

ì´ ì‹œìŠ¤í…œì€ **ê³ ë“±í•™êµ ìƒê¸°ë¶€ íŠ¹ê¸°ì‚¬í•­ ì‘ì„± ìë™ ê²€í† **ë¥¼ ìœ„í•œ AI ê¸°ë°˜ ë„êµ¬ì…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥
- âœ… **í™œë™ ì˜ì—­ë³„ íŠ¹ê¸°ì‚¬í•­ ê²€í† ** (ììœ¨/ìì¹˜í™œë™, ì§„ë¡œí™œë™)
- âœ… **í•™ì—… ìˆ˜ì¤€ë³„ ë§ì¶¤ í‰ê°€** (ìƒìœ„ê¶Œ, ì¤‘ìœ„ê¶Œ, í•˜ìœ„ê¶Œ)
- âœ… **AI ê¸°ë°˜ 3ë‹¨ê³„ í‰ê°€**
  - â‘  ì í•©ì„± í‰ê°€
  - â‘¡ ê²€í†  ì˜ê²¬ (ì¥ì /ë¶€ì¡±í•œ ì /ê°œì„  í¬ì¸íŠ¸)
  - â‘¢ ê°œì„  ì œì•ˆ (500ì ìˆ˜ì •ì•ˆ)
- âœ… **ì‹¤ì‹œê°„ ë¬¸ì¥ ë¶„ì„ ë° í”¼ë“œë°±**
- âœ… **ë²¡í„° ê²€ìƒ‰ ê¸°ë°˜ ì‘ì„±ìš”ë ¹ ë§¤ì¹­**

### ì‹œìŠ¤í…œ íŠ¹ì§•
- ğŸ”¥ **GPU ê°€ì† ì§€ì›** (CUDA + ONNX Runtime)
- ğŸ”„ **ì„¸ì…˜ ê¸°ë°˜ ë‹¤ì¤‘ ì‚¬ìš©ì ì§€ì›**
- ğŸ“š **ì˜ì—­ë³„ ì „ë¬¸ ê°€ì´ë“œë¼ì¸ ë°ì´í„°**
- ğŸŒ **ì›¹ ê¸°ë°˜ ì‚¬ìš©ì ì¹œí™”ì  ì¸í„°í˜ì´ìŠ¤**

---

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ & ì•„í‚¤í…ì²˜

### Backend
- **Python 3.12**
- **FastAPI** - ê³ ì„±ëŠ¥ ì›¹ API í”„ë ˆì„ì›Œí¬
- **LangChain** - AI ì²´ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- **Anthropic Claude** - ë¬¸ì¥ í‰ê°€ ë° ê°œì„ ì•ˆ ìƒì„±
- **ChromaDB** - ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
- **ONNX Runtime** - GPU ê°€ì† ì„ë² ë”© ëª¨ë¸ ì¶”ë¡ 
- **Transformers** - í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ (KoSimCSE ê¸°ë°˜)

### Frontend
- **React 18** + **TypeScript**
- **Chakra UI** - ëª¨ë˜ UI ì»´í¬ë„ŒíŠ¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
- **React Icons** - ì•„ì´ì½˜ ë¼ì´ë¸ŒëŸ¬ë¦¬

### AI & ML
- **KoSimCSE** - í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸
- **Vector Similarity Search** - RAG ê¸°ë°˜ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
- **GPU ìµœì í™”** - CUDA + ONNX Runtime í™œìš©

---

## ğŸ’» ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

### í•„ìˆ˜ ì†Œí”„íŠ¸ì›¨ì–´
- **Python 3.12** ì´ìƒ
- **Node.js 18** ì´ìƒ + npm
- **Git**
- **NVIDIA GPU** (ì„ íƒ, ì„±ëŠ¥ í–¥ìƒìš©)
- **CUDA Toolkit** (GPU ì‚¬ìš© ì‹œ)

### í•˜ë“œì›¨ì–´ ê¶Œì¥ì‚¬ì–‘
- **CPU**: 4ì½”ì–´ ì´ìƒ
- **RAM**: ìµœì†Œ 8GB (16GB ê¶Œì¥)
- **ì €ì¥ê³µê°„**: ìµœì†Œ 5GB (ëª¨ë¸ íŒŒì¼ í¬í•¨)
- **GPU**: NVIDIA RTX ì‹œë¦¬ì¦ˆ (ì„ íƒì‚¬í•­, ì„±ëŠ¥ í–¥ìƒ)

### ì§€ì› ìš´ì˜ì²´ì œ
- Windows 10/11
- macOS (Intel/Apple Silicon)
- Linux (Ubuntu 20.04+)

---

## ğŸ“¦ ì„¤ì¹˜ ë° ì‹¤í–‰ ê°€ì´ë“œ

### 1. í”„ë¡œì íŠ¸ ë³µì œ
```bash
git clone https://github.com/lifeprofessor/LifeRecordReviewSystem.git
cd LifeRecordReviewSystem
```

### 2. ë°±ì—”ë“œ ì„¤ì • ë° ì‹¤í–‰

#### 2.1 Python ê°€ìƒí™˜ê²½ ìƒì„± (ê¶Œì¥)
```bash
cd backend

# Windows
python -m venv venv
venv\Scripts\activate

# macOS/Linux
python3 -m venv venv
source venv/bin/activate
```

#### 2.2 ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
# ê¸°ë³¸ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (í•„ìˆ˜)
pip install -r requirements.txt

# GPU ê°€ì† íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ì„±ëŠ¥ í–¥ìƒìš©)
pip install -r requirements-gpu.txt

# ê°œë°œ í™˜ê²½ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (ê°œë°œììš©)
pip install -r requirements-dev.txt

# ë˜ëŠ” ë°±ì—”ë“œë§Œ ì„¤ì¹˜í•˜ëŠ” ê²½ìš°
pip install -r backend/requirements.txt
```

**íŒ¨í‚¤ì§€ íŒŒì¼ ì„¤ëª…:**
- `requirements.txt`: ê¸°ë³¸ í•„ìˆ˜ íŒ¨í‚¤ì§€ (í”„ë¡œë•ì…˜ìš©)
- `requirements-gpu.txt`: GPU ê°€ì† íŒ¨í‚¤ì§€ (ì„±ëŠ¥ í–¥ìƒìš©)
- `requirements-dev.txt`: ê°œë°œ ë„êµ¬ íŒ¨í‚¤ì§€ (ê°œë°œììš©)
- `backend/requirements.txt`: ë°±ì—”ë“œ ì „ìš© íŒ¨í‚¤ì§€

#### 2.3 í™˜ê²½ë³€ìˆ˜ ì„¤ì •
```bash
# .env íŒŒì¼ ìƒì„±
echo "ANTHROPIC_API_KEY=your_anthropic_api_key_here" > .env
```

#### 2.4 ë¡œì»¬ ëª¨ë¸ ì¤€ë¹„

ì‹œìŠ¤í…œì—ì„œ ì‚¬ìš©í•  í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

##### ğŸ“ ëª¨ë¸ í´ë” ìƒì„±
```bash
mkdir backend/model_files
```

##### ğŸ”— ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
1. **Hugging Face ëª¨ë¸ í˜ì´ì§€ ì ‘ì†**
   - [BM-K/KoSimCSE-roberta-multitask](https://huggingface.co/BM-K/KoSimCSE-roberta-multitask) ì ‘ì†

2. **íŒŒì¼ ë‹¤ìš´ë¡œë“œ**
   - í˜ì´ì§€ì—ì„œ **"Files"** íƒ­ í´ë¦­
   - ë‹¤ìŒ íŒŒì¼ë“¤ì„ `backend/model_files/` í´ë”ì— ë‹¤ìš´ë¡œë“œ:

| íŒŒì¼ëª… | ì„¤ëª… |
|--------|------|
| `config.json` | ëª¨ë¸ ì„¤ì • íŒŒì¼ |
| `pytorch_model.bin` | ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼ |
| `tokenizer.json` | í† í¬ë‚˜ì´ì € ì„¤ì • |
| `tokenizer_config.json` | í† í¬ë‚˜ì´ì € êµ¬ì„± |
| `vocab.txt` | ì–´íœ˜ ì‚¬ì „ |
| `special_tokens_map.json` | íŠ¹ìˆ˜ í† í° ë§¤í•‘ |

##### ğŸ“‚ í´ë” êµ¬ì¡° í™•ì¸
```bash
backend/model_files/
â”œâ”€â”€ config.json
â”œâ”€â”€ pytorch_model.bin
â”œâ”€â”€ tokenizer.json
â”œâ”€â”€ tokenizer_config.json
â”œâ”€â”€ vocab.txt
â””â”€â”€ special_tokens_map.json
```

##### ë‹¤ìš´ë¡œë“œ í™•ì¸
```bash
[ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸]
python -c "
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained('./backend/model_files', local_files_only=True)
model = AutoModel.from_pretrained('./backend/model_files', local_files_only=True)
print('âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!')
"
```

##### âœ… ì„¤ì¹˜ í™•ì¸
```bash
python -c "
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained('./backend/model_files', local_files_only=True)
model = AutoModel.from_pretrained('./backend/model_files', local_files_only=True)
print('âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ!')
"
```

#### 2.5 ë¡œì»¬ ëª¨ë¸ ë™ì‘ ê³¼ì •
ì‹œìŠ¤í…œì´ ì²˜ìŒ ì‹¤í–‰ë  ë•Œ ìë™ìœ¼ë¡œ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ONNXë¡œ ë³€í™˜í•©ë‹ˆë‹¤. 
ì´ ê³¼ì •ì€ ìµœì´ˆ 1íšŒë§Œ ì§„í–‰ë©ë‹ˆë‹¤.

#### 2.6 ë°±ì—”ë“œ ì„œë²„ ì‹¤í–‰
```bash
# ë¡œì»¬ ê°œë°œìš©
uvicorn main:app --host 127.0.0.1 --port 8000 --reload

# ë„¤íŠ¸ì›Œí¬ ì ‘ê·¼ í—ˆìš© (ë‚´ë¶€ë§ ë°°í¬ìš©)
uvicorn main:app --host 0.0.0.0 --port 8000

# GPU ê°€ì† ê°•ì œ í™œì„±í™”
FORCE_ONNX_MODE=true uvicorn main:app --host 0.0.0.0 --port 8000
```

### 3. í”„ë¡ íŠ¸ì—”ë“œ ì„¤ì • ë° ì‹¤í–‰

#### 3.1 ì˜ì¡´ì„± ì„¤ì¹˜
```bash
cd frontend
npm install
```

#### 3.2 ê°œë°œ ì„œë²„ ì‹¤í–‰
```bash
npm start
```

ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:3000` ì ‘ì†

---

## âš™ï¸ í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜

#### backend/.env
```env
# Anthropic API í‚¤ (í•„ìˆ˜)
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# ONNX ê°€ì† ëª¨ë“œ (ì„ íƒì‚¬í•­)
FORCE_ONNX_MODE=true

# SSL ì„¤ì • (ì„ íƒì‚¬í•­)
ANTHROPIC_VERIFY_SSL=false
```

### API í‚¤ ë°œê¸‰ ë°©ë²•
1. [Anthropic Console](https://console.anthropic.com/) ì ‘ì†
2. API í‚¤ ìƒì„±
3. `.env` íŒŒì¼ì— í‚¤ ì…ë ¥

### GPU ê°€ì† ì„¤ì •

#### cuDNN ì„¤ì¹˜ (GPU ê°€ì† í•„ìˆ˜)

cuDNNì€ ë”¥ëŸ¬ë‹ ì—°ì‚°ì„ GPUì—ì„œ ê°€ì†í•˜ëŠ” í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.

**1. NVIDIA ê°œë°œì ê³„ì • ìƒì„±**
- [NVIDIA Developer](https://developer.nvidia.com/) ì ‘ì† í›„ ë¬´ë£Œ ê°€ì…

**2. cuDNN ë‹¤ìš´ë¡œë“œ**
- [cuDNN ë‹¤ìš´ë¡œë“œ í˜ì´ì§€](https://developer.nvidia.com/cudnn) ì ‘ì†
- **CUDA 12.xìš© cuDNN 9.x** ì„ íƒ ë° ë‹¤ìš´ë¡œë“œ

**3. ì„¤ì¹˜ (Windows)**
```bash
# ë‹¤ìš´ë¡œë“œí•œ ì••ì¶• íŒŒì¼ ì••ì¶• í•´ì œ í›„ ë‹¤ìŒ íŒŒì¼ë“¤ì„ ë³µì‚¬:
# bin í´ë”ì˜ ëª¨ë“  DLL â†’ C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.x\bin
# include í´ë”ì˜ ëª¨ë“  íŒŒì¼ â†’ C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.x\include  
# lib í´ë”ì˜ ëª¨ë“  íŒŒì¼ â†’ C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.x\lib\x64
```

**4. í™˜ê²½ë³€ìˆ˜ ì„¤ì •**
- ì‹œìŠ¤í…œ í™˜ê²½ë³€ìˆ˜ Pathì— ì¶”ê°€: `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.x\bin`

**5. ì„¤ì¹˜ í™•ì¸**
```bash
# cuDNN ì„¤ì¹˜ í™•ì¸
python -c "import ctypes; ctypes.CDLL('cudnn64_9.dll')"

# ONNX Runtime GPU í™•ì¸
python -c "import onnxruntime as ort; print('Available providers:', ort.get_available_providers())"
# ì¶œë ¥ì— 'CUDAExecutionProvider'ê°€ ìˆì–´ì•¼ í•¨
```

```bash
# NVIDIA GPU ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# CUDA ë²„ì „ í™•ì¸
nvcc --version

# ONNX Runtime GPU ì„¤ì¹˜
pip install onnxruntime-gpu
```

---

## ğŸ”„ ì„œë¹„ìŠ¤ ë™ì‘ íë¦„

### 1. ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
```mermaid
graph TB
    A[ì‚¬ìš©ì] --> B[React Frontend]
    B --> C[FastAPI Backend]
    C --> D[ë²¡í„° ê²€ìƒ‰]
    C --> E[Claude API]
    D --> F[ChromaDB]
    E --> G[ë¬¸ì¥ í‰ê°€ ê²°ê³¼]
    
    subgraph "ë°±ì—”ë“œ ì²˜ë¦¬"
        H[ê°€ì´ë“œë¼ì¸ ë¬¸ì„œ] --> I[í…ìŠ¤íŠ¸ ë¶„í• ]
        I --> J[ì„ë² ë”© ìƒì„±]
        J --> F
        D --> K[ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰]
        K --> L[ì»¨í…ìŠ¤íŠ¸ ìƒì„±]
        L --> E
    end
```

### 2. ìƒì„¸ ì²˜ë¦¬ ê³¼ì •

#### ë‹¨ê³„ 1: ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„°í™”
```
ì‚¬ìš©ì ì„ íƒ (ì˜ì—­ + í•™ì—…ìˆ˜ì¤€)
    â†“
POST /api/load-documents
    â†“
í•´ë‹¹ ì˜ì—­ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ
    â†“
í…ìŠ¤íŠ¸ ë¶„í•  (500ì ë‹¨ìœ„, 50ì ê²¹ì¹¨)
    â†“
KoSimCSE ëª¨ë¸ë¡œ ì„ë² ë”© ìƒì„±
    â†“
ChromaDBì— ë²¡í„° ì €ì¥
    â†“
ì„¸ì…˜ ID ë°˜í™˜
```

#### ë‹¨ê³„ 2: ë¬¸ì¥ ê²€í†  ë° í‰ê°€
```
ì‚¬ìš©ì ë¬¸ì¥ ì…ë ¥
    â†“
POST /api/review
    â†“
ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ (ìƒìœ„ 3ê°œ)
    â†“
Claude API í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    â†“
AI í‰ê°€ ì‹¤í–‰
    â†“
ê²°ê³¼ íŒŒì‹± ë° ë°˜í™˜
    â†“
ì›¹ UIì— ê²°ê³¼ í‘œì‹œ
```

### 3. í”„ë¡ íŠ¸ì—”ë“œ ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°

#### App.tsx ì£¼ìš” ê¸°ëŠ¥
- **ì˜ì—­ ì„ íƒ**: ììœ¨/ìì¹˜í™œë™, ì§„ë¡œí™œë™
- **í•™ì—…ìˆ˜ì¤€ ì„ íƒ**: ìƒìœ„ê¶Œ, ì¤‘ìœ„ê¶Œ, í•˜ìœ„ê¶Œ  
- **ë¬¸ì„œ ë¡œë“œ**: ì„ íƒëœ ì˜ì—­ì˜ ê°€ì´ë“œë¼ì¸ ë¡œë“œ
- **ë¬¸ì¥ ê²€í† **: ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•œ AI í‰ê°€
- **ê²°ê³¼ í‘œì‹œ**: 3ë‹¨ê³„ í‰ê°€ ê²°ê³¼ (ì í•©ì„±/ê²€í† ì˜ê²¬/ê°œì„ ì œì•ˆ)

#### UI/UX íŠ¹ì§•
- **ë°˜ì‘í˜• ë””ìì¸**: ëª¨ë°”ì¼/íƒœë¸”ë¦¿/ë°ìŠ¤í¬í†± ì§€ì›
- **ì‹¤ì‹œê°„ í”¼ë“œë°±**: ë¡œë”© ìƒíƒœ ë° ì—ëŸ¬ ì²˜ë¦¬
- **ì§ê´€ì  ì¸í„°í˜ì´ìŠ¤**: ë‹¨ê³„ë³„ ì•ˆë‚´ì™€ ëª…í™•í•œ ê²°ê³¼ í‘œì‹œ

---

## ğŸ“¡ API ëª…ì„¸

### POST /api/load-documents
ë¬¸ì„œ ë¡œë“œ ë° ë²¡í„°í™”

**Request Body:**
```json
{
  "area": "ììœ¨/ìì¹˜í™œë™ íŠ¹ê¸°ì‚¬í•­" | "ì§„ë¡œí™œë™ íŠ¹ê¸°ì‚¬í•­",
  "academic_level": "ìƒìœ„ê¶Œ" | "ì¤‘ìœ„ê¶Œ" | "í•˜ìœ„ê¶Œ"
}
```

**Response:**
```json
{
  "status": "success",
  "message": "Documents loaded successfully with ONNX Runtime (GPU)",
  "session_id": "uuid-string",
  "server_info": {
    "active_sessions": 1,
    "processing_time": "2.3s",
    "acceleration": "ONNX Runtime (GPU)",
    "model_type": "ë¡œì»¬ ëª¨ë¸"
  }
}
```

### POST /api/review  
ë¬¸ì¥ ê²€í†  ë° í‰ê°€

**Request Body:**
```json
{
  "statement": "ê²€í† í•  íŠ¹ê¸°ì‚¬í•­ ë¬¸ì¥",
  "session_id": "uuid-string"
}
```

**Response:**
```json
{
  "evaluation": "ì í•©ì„± í‰ê°€ ê²°ê³¼",
  "feedback": "ğŸ’¡ ì¥ì \nğŸ‘‰ êµ¬ì²´ì ì¸ ì¥ì  ì„¤ëª…...",
  "suggestion": "ê°œì„ ëœ ë¬¸ì¥ (500ì)",
  "suggestion_length": 485
}
```

### GET /api/sessions
í™œì„± ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ

### DELETE /api/sessions/{session_id}
íŠ¹ì • ì„¸ì…˜ ì‚­ì œ

---

## â— ë¬¸ì œ í•´ê²°

### 1. íŒ¨í‚¤ì§€ ì„¤ì¹˜ ë¬¸ì œ

**ê°€ìƒí™˜ê²½ í™•ì¸**
```bash
# ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸
# í”„ë¡¬í”„íŠ¸ ì•ì— (venv) í‘œì‹œ í™•ì¸

# pip ì—…ê·¸ë ˆì´ë“œ
pip install --upgrade pip

# ìºì‹œ ë¬´ì‹œ ì„¤ì¹˜
pip install -r requirements.txt --no-cache-dir
```

**ONNX Runtime ì„¤ì¹˜ ì‹¤íŒ¨**
```bash
# GPU ë²„ì „ ì„¤ì¹˜ ì‹¤íŒ¨ ì‹œ CPU ë²„ì „ ì‚¬ìš©
pip uninstall onnxruntime-gpu
pip install onnxruntime optimum

# ë˜ëŠ” íŠ¹ì • ë²„ì „ ì§€ì •
pip install onnxruntime-gpu==1.15.1 optimum[onnxruntime]==1.14.0

# ì „ì²´ GPU íŒ¨í‚¤ì§€ ì¬ì„¤ì¹˜
pip uninstall -r requirements-gpu.txt
pip install onnxruntime optimum  # CPU ë²„ì „
```

### 2. ëª¨ë¸ ë¡œë”© ë¬¸ì œ

**ë¡œì»¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨**
```bash
# ì¸í„°ë„· ì—°ê²° í™•ì¸
ping huggingface.co

# ëª¨ë¸ ìºì‹œ ë””ë ‰í† ë¦¬ ê¶Œí•œ í™•ì¸
ls -la backend/model_files/
ls -la backend/model_cache/

# ìˆ˜ë™ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p backend/model_files
mkdir -p backend/model_cache
mkdir -p backend/onnx_models
```

**GPU ì¸ì‹ ë¬¸ì œ**
```bash
# NVIDIA ë“œë¼ì´ë²„ í™•ì¸
nvidia-smi

# CUDA ì„¤ì¹˜ í™•ì¸
python -c "import torch; print(torch.cuda.is_available())"

# ONNX Runtime GPU í™•ì¸
python -c "import onnxruntime as ort; print('CUDAExecutionProvider' in ort.get_available_providers())"
```

### 3. API ì—°ê²° ë¬¸ì œ

**Anthropic API í‚¤ ì˜¤ë¥˜**
```bash
# .env íŒŒì¼ ì¡´ì¬ í™•ì¸
ls -la backend/.env

# API í‚¤ í…ŒìŠ¤íŠ¸
curl -H "x-api-key: your_api_key" https://api.anthropic.com/v1/messages
```

**CORS ì—ëŸ¬**
- ë°±ì—”ë“œê°€ ì˜¬ë°”ë¥¸ CORS ì„¤ì •ìœ¼ë¡œ ì‹¤í–‰ë˜ê³  ìˆëŠ”ì§€ í™•ì¸
- í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì˜¬ë°”ë¥¸ API URL ì‚¬ìš© í™•ì¸ (`http://localhost:8000`)

### 4. ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ

**RAM ë¶€ì¡±**
```bash
# ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸° (main.py ìˆ˜ì •)
BATCH_SIZE = 8  # ê¸°ë³¸ê°’ 16ì—ì„œ ë³€ê²½

# ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
MAX_CONCURRENT_GPU_REQUESTS = 2
```

**GPU ë©”ëª¨ë¦¬ ë¶€ì¡±**
```bash
# CPU ëª¨ë“œë¡œ ê°•ì œ ì‹¤í–‰
FORCE_ONNX_MODE=false uvicorn main:app --host 0.0.0.0 --port 8000
```

---

## ğŸš€ ë°°í¬ ê°€ì´ë“œ

### ë¡œì»¬ ë„¤íŠ¸ì›Œí¬ ë°°í¬ (í•™êµ/ê¸°ê´€ ë‚´ë¶€ë§)

#### 1. ë°±ì—”ë“œ ë°°í¬
```bash
cd backend
# ëª¨ë“  ë„¤íŠ¸ì›Œí¬ ì¸í„°í˜ì´ìŠ¤ì—ì„œ ì ‘ê·¼ í—ˆìš©
uvicorn main:app --host 0.0.0.0 --port 8000
```

#### 2. í”„ë¡ íŠ¸ì—”ë“œ ë¹Œë“œ ë° ë°°í¬
```bash
cd frontend
# í”„ë¡œë•ì…˜ ë¹Œë“œ
npm run build

# ì •ì  ì„œë²„ ì„¤ì¹˜ ë° ì‹¤í–‰
npm install -g serve
serve -s build -l 3000
```

#### 3. ë„¤íŠ¸ì›Œí¬ ì ‘ê·¼
- **ë°±ì—”ë“œ**: `http://ì„œë²„IP:8000`
- **í”„ë¡ íŠ¸ì—”ë“œ**: `http://ì„œë²„IP:3000`
- **API ë¬¸ì„œ**: `http://ì„œë²„IP:8000/docs`

### ìë™ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

#### Windows (start_system.bat)
```batch
@echo off
echo Starting LifeRecordReview System...

cd backend
start cmd /k "venv\Scripts\activate && uvicorn main:app --host 0.0.0.0 --port 8000"

cd ..\frontend
start cmd /k "serve -s build -l 3000"

echo System started!
echo Backend: http://localhost:8000
echo Frontend: http://localhost:3000
pause
```

#### Linux/macOS (start_system.sh)
```bash
#!/bin/bash
echo "Starting LifeRecordReview System..."

# ë°±ì—”ë“œ ì‹¤í–‰
cd backend
source venv/bin/activate
uvicorn main:app --host 0.0.0.0 --port 8000 &

# í”„ë¡ íŠ¸ì—”ë“œ ì‹¤í–‰
cd ../frontend
serve -s build -l 3000 &

echo "System started!"
echo "Backend: http://localhost:8000"
echo "Frontend: http://localhost:3000"

wait
```

---

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### GPU ê°€ì† í™œìš©
- **ONNX Runtime + CUDA**: ìµœëŒ€ 5-10ë°° ì„±ëŠ¥ í–¥ìƒ
- **ë°°ì¹˜ ì²˜ë¦¬**: ë™ì‹œ ë‹¤ì¤‘ ìš”ì²­ ì²˜ë¦¬
- **ëª¨ë¸ ìºì‹±**: ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ëª¨ë¸ ê´€ë¦¬

### ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§
```python
# GPU ì‚¬ìš©ë¥  í™•ì¸
import pynvml
pynvml.nvmlInit()
handle = pynvml.nvmlDeviceGetHandleByIndex(0)
info = pynvml.nvmlDeviceGetMemoryInfo(handle)
print(f"GPU Memory: {info.used/1024**3:.1f}GB / {info.total/1024**3:.1f}GB")
```

---

## ğŸ”§ ê°œë°œì ì •ë³´

### í”„ë¡œì íŠ¸ êµ¬ì¡°
```
LifeRecordReviewSystem/
â”œâ”€â”€ backend/                    # FastAPI ë°±ì—”ë“œ
â”‚   â”œâ”€â”€ main.py                # ë©”ì¸ ì„œë²„ íŒŒì¼
â”‚   â”œâ”€â”€ requirements.txt       # ë°±ì—”ë“œ ì „ìš© ì˜ì¡´ì„±
â”‚   â”œâ”€â”€ data/                  # ê°€ì´ë“œë¼ì¸ ë°ì´í„°
â”‚   â”‚   â”œâ”€â”€ career_activity_guidelines/     # ì§„ë¡œí™œë™ (13ê°œ íŒŒì¼)
â”‚   â”‚   â””â”€â”€ self_governance_guidelines/     # ììœ¨/ìì¹˜í™œë™ (10ê°œ íŒŒì¼)
â”‚   â”œâ”€â”€ model_files/           # ë¡œì»¬ ì„ë² ë”© ëª¨ë¸
â”‚   â”œâ”€â”€ model_cache/           # ëª¨ë¸ ìºì‹œ
â”‚   â”œâ”€â”€ onnx_models/           # ONNX ë³€í™˜ ëª¨ë¸
â”‚   â””â”€â”€ chroma_db/            # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
â”œâ”€â”€ frontend/                  # React í”„ë¡ íŠ¸ì—”ë“œ
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx           # ë©”ì¸ ì»´í¬ë„ŒíŠ¸
â”‚   â”‚   â””â”€â”€ index.tsx         # ì§„ì…ì 
â”‚   â”œâ”€â”€ package.json          # Node.js ì˜ì¡´ì„±
â”‚   â””â”€â”€ public/               # ì •ì  íŒŒì¼
â”œâ”€â”€ requirements.txt          # ê¸°ë³¸ Python ì˜ì¡´ì„± (í•„ìˆ˜)
â”œâ”€â”€ requirements-gpu.txt      # GPU ê°€ì† íŒ¨í‚¤ì§€ (ì„ íƒ)
â”œâ”€â”€ requirements-dev.txt      # ê°œë°œ ë„êµ¬ íŒ¨í‚¤ì§€ (ê°œë°œììš©)
â””â”€â”€ README.md                # í”„ë¡œì íŠ¸ ë¬¸ì„œ
```

### ì£¼ìš” ë°ì´í„° íŒŒì¼
- **ììœ¨/ìì¹˜í™œë™**: 10ê°œ ë§ˆí¬ë‹¤ìš´ ê°€ì´ë“œë¼ì¸
- **ì§„ë¡œí™œë™**: 13ê°œ ë§ˆí¬ë‹¤ìš´ ê°€ì´ë“œë¼ì¸
- **ì´ 23ê°œ ì „ë¬¸ ì‘ì„±ìš”ë ¹ ë¬¸ì„œ** (ì•½ 150KB)

---

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” **MIT ë¼ì´ì„ ìŠ¤**ë¥¼ ë”°ë¦…ë‹ˆë‹¤.

---

## ğŸ™‹â€â™‚ï¸ ì§€ì› ë° ë¬¸ì˜

ë¬¸ì œ ë°œìƒ ì‹œ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì´ìŠˆë¥¼ ë“±ë¡í•´ì£¼ì„¸ìš”:

1. **ìš´ì˜ì²´ì œ ë° ë²„ì „**
2. **Python ë²„ì „**
3. **GPU ì‚¬ìš© ì—¬ë¶€**
4. **ì—ëŸ¬ ë©”ì‹œì§€ ì „ë¬¸**
5. **ì¬í˜„ ë°©ë²•**

---

**ğŸ¯ ì´ ì‹œìŠ¤í…œìœ¼ë¡œ ë” ë‚˜ì€ ìƒê¸°ë¶€ íŠ¹ê¸°ì‚¬í•­ì„ ì‘ì„±í•˜ì„¸ìš”!**
