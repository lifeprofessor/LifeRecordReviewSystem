from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, SecretStr
import os
from typing import Optional, List, Dict
from langchain_community.vectorstores import Chroma
from langchain_anthropic import ChatAnthropic
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema import Document
from langchain.text_splitter import MarkdownTextSplitter
from langchain.embeddings.base import Embeddings
from dotenv import load_dotenv
import ssl
import gc
import re
import uuid
from datetime import datetime, timedelta
import urllib3
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from urllib3.poolmanager import PoolManager
import asyncio
from asyncio import Semaphore
import threading
import time
import numpy as np
import atexit
import signal

# ONNX Runtime ê´€ë ¨ import (ì•ˆì „í•œ ë°©ì‹)
try:
    import onnxruntime as ort
    ONNX_AVAILABLE = True
except ImportError:
    print("âš ï¸ ONNX Runtimeì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    ONNX_AVAILABLE = False

# Transformers import (ì•ˆì „í•œ ë°©ì‹)
try:
    from transformers.models.auto.tokenization_auto import AutoTokenizer
    TRANSFORMERS_AVAILABLE = True
except ImportError:
    print("âš ï¸ Transformers íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    TRANSFORMERS_AVAILABLE = False

# Optimum import (ì•ˆì „í•œ ë°©ì‹)
try:
    from optimum.onnxruntime import ORTModelForFeatureExtraction
    OPTIMUM_AVAILABLE = True
except ImportError:
    print("âš ï¸ Optimum íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    OPTIMUM_AVAILABLE = False

# pynvml import (ì„ íƒì‚¬í•­)
try:
    import pynvml
    PYNVML_AVAILABLE = True
except ImportError:
    print("ğŸ“ pynvmlì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GPU ì •ë³´ í‘œì‹œê°€ ì œí•œë©ë‹ˆë‹¤.")
    PYNVML_AVAILABLE = False

# PyTorch import (ë¡œì»¬ ëª¨ë¸ìš©)
try:
    import torch
    TORCH_AVAILABLE = True
except ImportError:
    print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¡œì»¬ ëª¨ë¸ ì‚¬ìš©ì´ ì œí•œë©ë‹ˆë‹¤.")
    TORCH_AVAILABLE = False

# httpx ëª¨ë“ˆ ì „ì—­ íŒ¨ì¹˜
import httpx

_original_client_init = httpx.Client.__init__
_original_async_client_init = httpx.AsyncClient.__init__

def _patched_client_init(self, *args, **kwargs):
    kwargs['verify'] = False
    kwargs.setdefault('timeout', 60.0)
    return _original_client_init(self, *args, **kwargs)

def _patched_async_client_init(self, *args, **kwargs):
    kwargs['verify'] = False
    kwargs.setdefault('timeout', 60.0)
    return _original_async_client_init(self, *args, **kwargs)

httpx.Client.__init__ = _patched_client_init
httpx.AsyncClient.__init__ = _patched_async_client_init

ssl._create_default_https_context = ssl._create_unverified_context
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

class CustomHTTPAdapter(HTTPAdapter):
    def init_poolmanager(self, connections, maxsize, block=False):
        ctx = ssl.create_default_context()
        ctx.check_hostname = False
        ctx.verify_mode = ssl.CERT_NONE
        self.poolmanager = PoolManager(
            num_pools=connections,
            maxsize=maxsize,
            block=block,
            ssl_version=ssl.PROTOCOL_TLS,
            ssl_context=ctx
        )

session = requests.Session()
adapter = CustomHTTPAdapter()
session.mount("https://", adapter)
session.mount("http://", adapter)
requests.Session = lambda: session

cert_path = "C:\\cert\\sdj_ssl.crt"
if os.path.exists(cert_path):
    os.environ['CURL_CA_BUNDLE'] = cert_path
    os.environ['REQUESTS_CA_BUNDLE'] = cert_path
    os.environ['SSL_CERT_FILE'] = cert_path
    os.environ['SSL_CERT_DIR'] = os.path.dirname(cert_path)
else:
    print(f"Warning: Certificate file not found at {cert_path}")
    os.environ['CURL_CA_BUNDLE'] = ''
    os.environ['REQUESTS_CA_BUNDLE'] = ''
    os.environ['SSL_CERT_FILE'] = ''
    os.environ['SSL_CERT_DIR'] = ''

os.environ['PYTHONHTTPSVERIFY'] = '0'

load_dotenv()

# Configuration constants
# ë¡œì»¬ ëª¨ë¸ ê²½ë¡œ (ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½)
LOCAL_MODEL_PATH = os.path.abspath("./model_files")

CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
SEARCH_K = 3
CHROMA_DB_DIR = "./chroma_db"
MODEL_CACHE_DIR = "./model_cache"
ONNX_MODEL_DIR = "./onnx_models"
SESSION_TIMEOUT = timedelta(hours=1)

# ONNX ê°•ì œ ì‚¬ìš© í”Œë˜ê·¸ (RTX 5070 sm_120 ë¬¸ì œ í•´ê²°ìš©)
FORCE_ONNX_MODE = os.getenv("FORCE_ONNX_MODE", "true").lower() == "true"

# ë¡œì»¬ ëª¨ë¸ë§Œ ì‚¬ìš© (ì˜¨ë¼ì¸ ëª¨ë¸ ì œê±°)
EMBEDDING_MODEL = LOCAL_MODEL_PATH

MAX_CONCURRENT_GPU_REQUESTS = 5
BATCH_SIZE = 16
gpu_semaphore = Semaphore(MAX_CONCURRENT_GPU_REQUESTS)

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class LoadDocumentRequest(BaseModel):
    area: str
    academic_level: str

class ReviewRequest(BaseModel):
    statement: str
    session_id: str

class ReviewResponse(BaseModel):
    evaluation: str
    feedback: str
    suggestion: str
    suggestion_length: int

class SessionInfo(BaseModel):
    session_id: str
    created_at: datetime
    area: str
    academic_level: str

# Global variables
tokenizer = None
ort_session = None
sessions: Dict[str, dict] = {}

def check_cuda_availability():
    """CUDA ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (ì•ˆì „í•œ ë°©ì‹)"""
    if not ONNX_AVAILABLE:
        return False, None, None
        
    try:
        providers = ort.get_available_providers()
        cuda_available = 'CUDAExecutionProvider' in providers
        
        if cuda_available:
            if PYNVML_AVAILABLE:
                try:
                    pynvml.nvmlInit()
                    handle = pynvml.nvmlDeviceGetHandleByIndex(0)
                    gpu_name = pynvml.nvmlDeviceGetName(handle).decode('utf-8')
                    memory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)
                    total_memory = int(memory_info.total) / (1024**3)
                    return True, gpu_name, total_memory
                except Exception as e:
                    print(f"pynvml ì˜¤ë¥˜: {str(e)}")
                    return True, "NVIDIA GPU", "ì•Œ ìˆ˜ ì—†ìŒ"
            else:
                return True, "NVIDIA GPU", "ì•Œ ìˆ˜ ì—†ìŒ"
        else:
            return False, None, None
    except Exception as e:
        print(f"CUDA í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False, None, None

def setup_onnx_providers():
    """ONNX Runtime í”„ë¡œë°”ì´ë” ì„¤ì •"""
    if not ONNX_AVAILABLE:
        raise Exception("ONNX Runtimeì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    cuda_available, gpu_name, gpu_memory = check_cuda_availability()
    
    if cuda_available:
        print(f"ğŸš€ CUDA GPU ì‚¬ìš©: {gpu_name}")
        if gpu_memory != "ì•Œ ìˆ˜ ì—†ìŒ":
            print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f}GB")
        print(f"ğŸ‘¥ ìµœëŒ€ ë™ì‹œ ì‚¬ìš©ì: {MAX_CONCURRENT_GPU_REQUESTS}")
        print(f"ğŸ“¦ ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°: {BATCH_SIZE}")
        
        providers = [
            ('CUDAExecutionProvider', {
                'device_id': 0,
                'arena_extend_strategy': 'kNextPowerOfTwo',
                'gpu_mem_limit': int(2 * 1024 * 1024 * 1024),
                'cudnn_conv_algo_search': 'EXHAUSTIVE',
                'do_copy_in_default_stream': True,
            }),
            'CPUExecutionProvider'
        ]
    else:
        print("ğŸ–¥ï¸ CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        providers = ['CPUExecutionProvider']
    
    return providers

def download_and_cache_model():
    """ë¡œì»¬ ëª¨ë¸ì„ ì§ì ‘ ONNXë¡œ ë³€í™˜ (SentenceTransformer ìš°íšŒ)"""
    global tokenizer, ort_session
    
    # ë¡œì»¬ ëª¨ë¸ ì¡´ì¬ í™•ì¸
    if not os.path.exists(EMBEDDING_MODEL):
        raise HTTPException(status_code=500, detail=f"ë¡œì»¬ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {EMBEDDING_MODEL}")
    
    try:
        print("ğŸ”„ ë¡œì»¬ ëª¨ë¸ ì§ì ‘ ONNX ë³€í™˜ ì¤‘...")
        print(f"ğŸ“ ëª¨ë¸ ê²½ë¡œ: {EMBEDDING_MODEL}")
        
        # ONNX í•„ìˆ˜ íŒ¨í‚¤ì§€ í™•ì¸
        if not ONNX_AVAILABLE or not OPTIMUM_AVAILABLE:
            raise Exception("ONNX Runtime ë˜ëŠ” Optimum íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        for dir_path in [MODEL_CACHE_DIR, ONNX_MODEL_DIR]:
            if not os.path.exists(dir_path):
                os.makedirs(dir_path)
        
        # í† í¬ë‚˜ì´ì € ì§ì ‘ ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL, local_files_only=True)
        
        # ONNX ëª¨ë¸ ë¡œë“œ ë˜ëŠ” ìƒì„±
        onnx_model_path = os.path.join(ONNX_MODEL_DIR, "model.onnx")
        
        if not os.path.exists(onnx_model_path):
            print("ğŸ“¦ Transformers ëª¨ë¸ì„ ì§ì ‘ ONNXë¡œ ë³€í™˜ ì¤‘...")
            
            # *** í•µì‹¬: SentenceTransformer ëŒ€ì‹  ì§ì ‘ ë³€í™˜ ***
            from transformers.models.auto.modeling_auto import AutoModel
            
            # PyTorch ëª¨ë¸ ë¡œë“œ
            pytorch_model = AutoModel.from_pretrained(EMBEDDING_MODEL, local_files_only=True)
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ëª¨ë¸ ì €ì¥ (ONNX ë³€í™˜ìš©)
            temp_model_dir = os.path.join(MODEL_CACHE_DIR, "temp_for_onnx")
            os.makedirs(temp_model_dir, exist_ok=True)
            
            pytorch_model.save_pretrained(temp_model_dir)
            tokenizer.save_pretrained(temp_model_dir)
            
            # ONNX ë³€í™˜
            model = ORTModelForFeatureExtraction.from_pretrained(
                temp_model_dir,
                export=True,
                local_files_only=True
            )
            model.save_pretrained(ONNX_MODEL_DIR)
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ì •ë¦¬
            import shutil
            shutil.rmtree(temp_model_dir)
            
        else:
            print("ğŸ“ ê¸°ì¡´ ONNX ëª¨ë¸ ë¡œë“œ ì¤‘...")
            model = ORTModelForFeatureExtraction.from_pretrained(ONNX_MODEL_DIR)
        
        # ONNX Runtime ì„¸ì…˜ ìƒì„±
        providers = setup_onnx_providers()
        
        onnx_files = [f for f in os.listdir(ONNX_MODEL_DIR) if f.endswith('.onnx')]
        if onnx_files:
            actual_onnx_path = os.path.join(ONNX_MODEL_DIR, onnx_files[0])
        else:
            actual_onnx_path = model.model_path
        
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        ort_session = ort.InferenceSession(
            actual_onnx_path,
            sess_options,
            providers=providers
        )
        
        print("âœ… ë¡œì»¬ ëª¨ë¸ ONNX+GPU ë³€í™˜ ì™„ë£Œ!")
        print(f"ğŸ”§ ì‚¬ìš© ì¤‘ì¸ í”„ë¡œë°”ì´ë”: {ort_session.get_providers()}")
        
        return tokenizer, ort_session
        
    except Exception as e:
        print(f"ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì§ì ‘ ONNX ë³€í™˜ ì¤‘ ì˜¤ë¥˜: {str(e)}")

async def get_embeddings_batch_unified(texts, tokenizer, model_or_session):
    """PyTorchì™€ ONNX ëª¨ë‘ ì§€ì›í•˜ëŠ” í†µí•© ì„ë² ë”© ìƒì„±"""
    async with gpu_semaphore:
        try:
            all_embeddings = []
            
            # ONNX Runtime ì„¸ì…˜ì¸ì§€ PyTorch ëª¨ë¸ì¸ì§€ í™•ì¸
            is_onnx = hasattr(model_or_session, 'run')  # ONNX ì„¸ì…˜ì€ run ë©”ì„œë“œê°€ ìˆìŒ
            
            for i in range(0, len(texts), BATCH_SIZE):
                batch_texts = texts[i:i + BATCH_SIZE]
                
                if is_onnx:
                    # ONNX Runtime ë°©ì‹
                    inputs = tokenizer(
                        batch_texts,
                        padding=True,
                        truncation=True,
                        max_length=512,
                        return_tensors="np"
                    )
                    
                    ort_inputs = {
                        'input_ids': inputs['input_ids'].astype(np.int64),
                        'attention_mask': inputs['attention_mask'].astype(np.int64)
                    }
                    
                    outputs = model_or_session.run(None, ort_inputs)
                    last_hidden_state = outputs[0]
                    
                    attention_mask = inputs['attention_mask']
                    mask_expanded = np.expand_dims(attention_mask, axis=-1)
                    mask_expanded = np.broadcast_to(mask_expanded, last_hidden_state.shape)
                    
                    sum_embeddings = np.sum(last_hidden_state * mask_expanded, axis=1)
                    sum_mask = np.sum(mask_expanded, axis=1)
                    sum_mask = np.clip(sum_mask, a_min=1e-9, a_max=None)
                    
                    embeddings = sum_embeddings / sum_mask
                    
                    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
                    norms = np.clip(norms, a_min=1e-9, a_max=None)
                    embeddings = embeddings / norms
                    
                    batch_embeddings = embeddings.tolist()
                    
                else:
                    # PyTorch ë°©ì‹
                    import torch
                    
                    device = next(model_or_session.parameters()).device
                    
                    inputs = tokenizer(
                        batch_texts,
                        padding=True,
                        truncation=True,
                        max_length=512,
                        return_tensors="pt"
                    )
                    
                    # GPUë¡œ ì´ë™
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = model_or_session(**inputs)
                        last_hidden_state = outputs.last_hidden_state
                        
                        # Mean pooling
                        attention_mask = inputs['attention_mask']
                        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
                        
                        sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)
                        sum_mask = torch.sum(mask_expanded, 1)
                        sum_mask = torch.clamp(sum_mask, min=1e-9)
                        
                        embeddings = sum_embeddings / sum_mask
                        
                        # L2 ì •ê·œí™”
                        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
                        
                        # CPUë¡œ ì´ë™ í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
                        batch_embeddings = embeddings.cpu().tolist()
                
                all_embeddings.extend(batch_embeddings)
            
            return all_embeddings
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

async def get_embeddings_unified(text, tokenizer, model_or_session):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ìš© í†µí•© ì„ë² ë”©"""
    results = await get_embeddings_batch_unified([text], tokenizer, model_or_session)
    return results[0]

class UnifiedEmbeddingFunction(Embeddings):
    def __init__(self, tokenizer, model_or_session):
        self.tokenizer = tokenizer
        self.model_or_session = model_or_session

    def embed_documents(self, texts):
        """ë™ê¸° ë°©ì‹ ë¬¸ì„œ ì„ë² ë”© (ChromaDB í˜¸í™˜)"""
        try:
            return self._get_embeddings_batch_sync(texts)
        except Exception as e:
            print(f"ë¬¸ì„œ ì„ë² ë”© ì˜¤ë¥˜: {str(e)}")
            raise

    def embed_query(self, text):
        """ë™ê¸° ë°©ì‹ ì¿¼ë¦¬ ì„ë² ë”© (ChromaDB í˜¸í™˜)"""
        try:
            results = self._get_embeddings_batch_sync([text])
            return results[0]
        except Exception as e:
            print(f"ì¿¼ë¦¬ ì„ë² ë”© ì˜¤ë¥˜: {str(e)}")
            raise

    def _get_embeddings_batch_sync(self, texts):
        """ë™ê¸° ë°©ì‹ ë°°ì¹˜ ì„ë² ë”© ìƒì„±"""
        try:
            all_embeddings = []
            
            # ONNX Runtime ì„¸ì…˜ì¸ì§€ PyTorch ëª¨ë¸ì¸ì§€ í™•ì¸
            is_onnx = hasattr(self.model_or_session, 'run')
            
            for i in range(0, len(texts), BATCH_SIZE):
                batch_texts = texts[i:i + BATCH_SIZE]
                
                if is_onnx:
                    # ONNX Runtime ë°©ì‹
                    inputs = self.tokenizer(
                        batch_texts,
                        padding=True,
                        truncation=True,
                        max_length=512,
                        return_tensors="np"
                    )
                    
                    ort_inputs = {
                        'input_ids': inputs['input_ids'].astype(np.int64),
                        'attention_mask': inputs['attention_mask'].astype(np.int64)
                    }
                    
                    outputs = self.model_or_session.run(None, ort_inputs)
                    last_hidden_state = outputs[0]
                    
                    attention_mask = inputs['attention_mask']
                    mask_expanded = np.expand_dims(attention_mask, axis=-1)
                    mask_expanded = np.broadcast_to(mask_expanded, last_hidden_state.shape)
                    
                    sum_embeddings = np.sum(last_hidden_state * mask_expanded, axis=1)
                    sum_mask = np.sum(mask_expanded, axis=1)
                    sum_mask = np.clip(sum_mask, a_min=1e-9, a_max=None)
                    
                    embeddings = sum_embeddings / sum_mask
                    
                    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)
                    norms = np.clip(norms, a_min=1e-9, a_max=None)
                    embeddings = embeddings / norms
                    
                    batch_embeddings = embeddings.tolist()
                    
                else:
                    # PyTorch ë°©ì‹
                    import torch
                    
                    device = next(self.model_or_session.parameters()).device
                    
                    inputs = self.tokenizer(
                        batch_texts,
                        padding=True,
                        truncation=True,
                        max_length=512,
                        return_tensors="pt"
                    )
                    
                    # GPUë¡œ ì´ë™
                    inputs = {k: v.to(device) for k, v in inputs.items()}
                    
                    with torch.no_grad():
                        outputs = self.model_or_session(**inputs)
                        last_hidden_state = outputs.last_hidden_state
                        
                        # Mean pooling
                        attention_mask = inputs['attention_mask']
                        mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
                        
                        sum_embeddings = torch.sum(last_hidden_state * mask_expanded, 1)
                        sum_mask = torch.sum(mask_expanded, 1)
                        sum_mask = torch.clamp(sum_mask, min=1e-9)
                        
                        embeddings = sum_embeddings / sum_mask
                        
                        # L2 ì •ê·œí™”
                        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
                        
                        # CPUë¡œ ì´ë™ í›„ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
                        batch_embeddings = embeddings.cpu().tolist()
                
                all_embeddings.extend(batch_embeddings)
            
            return all_embeddings
            
        except Exception as e:
            raise Exception(f"ì„ë² ë”© ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

def cleanup_expired_sessions():
    current_time = datetime.now()
    expired_sessions = [
        session_id for session_id, session_data in sessions.items()
        if current_time - session_data['created_at'] > SESSION_TIMEOUT
    ]
    
    for session_id in expired_sessions:
        try:
            # 1. vectorstore ê°ì²´ ë¨¼ì € ì •ë¦¬
            if session_id in sessions and 'vectorstore' in sessions[session_id]:
                vectorstore = sessions[session_id]['vectorstore']
                if hasattr(vectorstore, '_client') and vectorstore._client:
                    try:
                        vectorstore._client.reset()
                    except:
                        pass
                del vectorstore
            
            # 2. ì„¸ì…˜ì—ì„œ ì œê±°
            if session_id in sessions:
                del sessions[session_id]
            
            # 3. íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì‚­ì œ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨)
            session_dir = os.path.join(CHROMA_DB_DIR, session_id)
            if os.path.exists(session_dir):
                import shutil
                import time
                
                # Windowsì—ì„œì˜ íŒŒì¼ ì‚­ì œ ì¬ì‹œë„
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
                        import gc
                        gc.collect()
                        
                        # ì ì‹œ ëŒ€ê¸° í›„ ì‚­ì œ ì‹œë„
                        if attempt > 0:
                            time.sleep(0.5)
                        
                        shutil.rmtree(session_dir)
                        print(f"âœ… ë§Œë£Œëœ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ì‚­ì œ ì™„ë£Œ: {session_id}")
                        break
                        
                    except PermissionError as e:
                        if attempt == max_retries - 1:
                            print(f"âš ï¸ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ì‚­ì œ ì‹¤íŒ¨ (ê¶Œí•œ ë¬¸ì œ): {session_id} - {str(e)}")
                            # ì‚­ì œ ì‹¤íŒ¨ ì‹œì—ë„ ì„¸ì…˜ì€ ë©”ëª¨ë¦¬ì—ì„œ ì œê±°í–ˆìœ¼ë¯€ë¡œ ê³„ì† ì§„í–‰
                        else:
                            print(f"ğŸ”„ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ì‚­ì œ ì¬ì‹œë„ ì¤‘... ({attempt + 1}/{max_retries}): {session_id}")
                    except Exception as e:
                        print(f"âŒ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ì‚­ì œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {session_id} - {str(e)}")
                        break
                        
        except Exception as e:
            print(f"âŒ ì„¸ì…˜ ì •ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {session_id} - {str(e)}")
            # ê°œë³„ ì„¸ì…˜ ì •ë¦¬ ì‹¤íŒ¨ ì‹œì—ë„ ë‹¤ë¥¸ ì„¸ì…˜ë“¤ì€ ê³„ì† ì²˜ë¦¬
            continue

@app.post("/api/load-documents")
async def load_documents(request: LoadDocumentRequest):
    try:
        cleanup_expired_sessions()
        
        area_map = {
            "ììœ¨/ìì¹˜í™œë™ íŠ¹ê¸°ì‚¬í•­": "self_governance_guidelines",
            "ì§„ë¡œí™œë™ íŠ¹ê¸°ì‚¬í•­": "career_activity_guidelines"
        }
        
        if request.area not in area_map:
            raise HTTPException(status_code=400, detail=f"Invalid area selected: {request.area}")
        
        directory = f"data/{area_map[request.area]}"
        if not os.path.exists(directory):
            raise HTTPException(status_code=404, detail=f"Directory not found: {directory}")
        
        if not os.listdir(directory):
            raise HTTPException(status_code=404, detail=f"Directory is empty: {directory}")
        
        # Generate session ID
        session_id = str(uuid.uuid4())
        session_db_dir = os.path.join(CHROMA_DB_DIR, session_id)
        
        # Load documents
        documents = []
        for file_path in os.listdir(directory):
            try:
                if file_path.endswith('.md'):
                    with open(os.path.join(directory, file_path), 'r', encoding='utf-8') as f:
                        content = f.read()
                        documents.append(Document(page_content=content, metadata={"source": file_path}))
            except Exception as e:
                print(f"Error reading file {file_path}: {str(e)}")
        
        if len(documents) == 0:
            raise HTTPException(status_code=404, detail=f"No markdown files found in {directory}")
        
        # Split text
        text_splitter = MarkdownTextSplitter(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP
        )
        splits = text_splitter.split_documents(documents)
        
        # Initialize models if needed
        if tokenizer is None or ort_session is None:
            try:
                download_and_cache_model()
            except Exception as e:
                raise HTTPException(status_code=500, detail=f"Error downloading model: {str(e)}")
        
        # í†µí•© ì„ë² ë”© í•¨ìˆ˜ ì‚¬ìš© (PyTorch/ONNX ìë™ ê°ì§€)
        embedding_function = UnifiedEmbeddingFunction(tokenizer, ort_session)
        
        try:
            os.makedirs(session_db_dir, exist_ok=True)
            
            # ì‚¬ìš© ì¤‘ì¸ ëª¨ë¸ íƒ€ì… í™•ì¸
            is_local_model = (os.path.exists(EMBEDDING_MODEL) and 
                            os.path.isdir(EMBEDDING_MODEL) and
                            os.path.exists(os.path.join(EMBEDDING_MODEL, "config.json")))
            
            # ê°€ì† íƒ€ì… ê²°ì •
            if FORCE_ONNX_MODE:
                acceleration_type = "ONNX Runtime (GPU)"
            elif hasattr(ort_session, 'run'):  # ONNX ì„¸ì…˜
                acceleration_type = "ONNX Runtime"
            else:  # PyTorch ëª¨ë¸
                acceleration_type = "PyTorch (ë¡œì»¬)"
            
            print(f"ğŸ“Š {acceleration_type}ë¡œ ë¬¸ì„œ {len(splits)}ê°œ ì²˜ë¦¬ ì¤‘... (ì‚¬ìš©ì: {len(sessions)+1}ëª…)")
            start_time = time.time()
            
            vectorstore = Chroma.from_documents(
                documents=splits,
                embedding=embedding_function,
                persist_directory=session_db_dir
            )
            
            end_time = time.time()
            print(f"âš¡ {acceleration_type} ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {end_time - start_time:.1f}ì´ˆ")
            
            # Store session information
            sessions[session_id] = {
                'vectorstore': vectorstore,
                'created_at': datetime.now(),
                'area': request.area,
                'academic_level': request.academic_level
            }
            
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Error creating vectorstore: {str(e)}")
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬
        gc.collect()
        
        active_sessions = len(sessions)
        
        return {
            "status": "success",
            "message": f"Documents loaded successfully with {acceleration_type}",
            "session_id": session_id,
            "server_info": {
                "active_sessions": active_sessions,
                "processing_time": f"{end_time - start_time:.1f}s",
                "acceleration": acceleration_type,
                "model_type": "ë¡œì»¬ ëª¨ë¸" if is_local_model else "ì˜¨ë¼ì¸ ëª¨ë¸"
            }
        }

    except Exception as e:
        import traceback
        print("--- LOAD DOCUMENTS ENDPOINT ERROR ---")
        traceback.print_exc()
        print("-------------------------------------")
        raise HTTPException(status_code=500, detail=f"An unexpected error occurred: {str(e)}")

@app.post("/api/review")
async def review_statement(request: ReviewRequest):
    cleanup_expired_sessions()  # ë§Œë£Œëœ ì„¸ì…˜ ì •ë¦¬
    
    if request.session_id not in sessions:
        raise HTTPException(status_code=400, detail="Invalid or expired session ID")
    
    session_data = sessions[request.session_id]
    vectorstore = session_data['vectorstore']
    
    if not request.statement:
        raise HTTPException(status_code=400, detail="Statement is required")
    
    try:
        template = """
ë‹¹ì‹ ì€ ê³ ë“±í•™êµ ìƒê¸°ë¶€ íŠ¹ê¸°ì‚¬í•­ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ ì…ë ¥ëœ ë¬¸ì¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ì„¸ ê°€ì§€ í•­ëª©ì„ í‰ê°€í•´ ì£¼ì„¸ìš”:

â‘  ì í•©ì„± í‰ê°€: í•´ë‹¹ ë¬¸ì¥ì´ ì‘ì„±ìš”ë ¹ì— ì í•©í•œì§€ ê°„ë‹¨íˆ íŒë‹¨í•´ ì£¼ì„¸ìš”.
â‘¡ ê²€í†  ì˜ê²¬: ë¬¸ì¥ì˜ ì¥ì , ë¶€ì¡±í•œ ì , ê°œì„  í¬ì¸íŠ¸ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
â‘¢ ê°œì„  ì œì•ˆ: ì‘ì„±ìš”ë ¹ì„ ê³ ë ¤í•˜ì—¬ ë¬¸ì¥ì„ ë” ë‚˜ì€ í˜•íƒœì˜ 500ìë¡œ ìˆ˜ì •í•´ ì£¼ì„¸ìš”

ì…ë ¥ ë¬¸ì¥:
{question}

â€» ì‘ë‹µì€ ë°˜ë“œì‹œ ìœ„ ìˆœì„œì™€ ë²ˆí˜¸(â‘ â‘¡â‘¢)ë¥¼ í¬í•¨í•˜ì—¬ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.
        """

        print("=== REVIEW API DEBUG ===")
        print(f"Session ID: {request.session_id}")
        print(f"Statement: {request.statement}")
        print(f"Vectorstore type: {type(vectorstore)}")
        
        try:
            chain = create_chain(vectorstore)
            print("Chain created successfully")
        except Exception as chain_error:
            print(f"Chain creation error: {str(chain_error)}")
            raise HTTPException(status_code=500, detail=f"Chain creation failed: {str(chain_error)}")
        
        print("Claude API í˜¸ì¶œ í”„ë¡¬í”„íŠ¸ ì „ì²´:")
        print(template.replace("{question}", request.statement))
        
        try:
            response = chain.invoke(request.statement)
            print("Chain invoke completed")
        except Exception as invoke_error:
            print(f"Chain invoke error: {str(invoke_error)}")
            raise HTTPException(status_code=500, detail=f"Chain invoke failed: {str(invoke_error)}")
        
        # Claude ì‘ë‹µì—ì„œ content ì¶”ì¶œ
        result = getattr(response, "content", None)
        if not result and isinstance(response, dict):
            result = response.get("content", "")
        elif not result:
            result = str(response)

        print("Claude API ì‘ë‹µ ì›ë¬¸:\n", result)

        # ê° í•­ëª© íŒŒì‹±
        eval_part, feedback_part, suggestion_part = "", "", ""
        try:
            parts = result.split("â‘ ")[1].split("â‘¡")
            eval_part = parts[0].strip()
            feedback_suggestion = parts[1].split("â‘¢")
            feedback_part = feedback_suggestion[0].strip()
            suggestion_part = feedback_suggestion[1].strip()
        except Exception as parse_error:
            print(f"Parsing error: {str(parse_error)}")
            eval_part = result.strip()

        # íŒŒì‹± í›„
        eval_part = remove_heading(eval_part, "ì í•©ì„± í‰ê°€")
        feedback_part = remove_heading(feedback_part, "ê²€í†  ì˜ê²¬")
        suggestion_part = remove_heading(suggestion_part, "ê°œì„  ì œì•ˆ")

        # ê°€ë…ì„± ê°œì„ 
        eval_part = prettify_evaluation(eval_part)
        feedback_part = prettify_feedback(feedback_part)
        suggestion_part = prettify_suggestion(suggestion_part)

        # 500ì ì œí•œ (ê³µë°± í¬í•¨)
        suggestion_part = suggestion_part[:500]
        suggestion_length = len(suggestion_part)

        print("=== REVIEW API SUCCESS ===")
        return {
            "evaluation": eval_part,
            "feedback": feedback_part,
            "suggestion": suggestion_part,
            "suggestion_length": suggestion_length
        }

    except Exception as e:
        import traceback
        print("=== REVIEW API ERROR ===")
        print(f"Error: {str(e)}")
        traceback.print_exc()
        print("========================")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/sessions")
async def list_sessions():
    """í˜„ì¬ í™œì„±í™”ëœ ì„¸ì…˜ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    cleanup_expired_sessions()
    return [
        SessionInfo(
            session_id=session_id,
            created_at=session_data['created_at'],
            area=session_data['area'],
            academic_level=session_data['academic_level']
        )
        for session_id, session_data in sessions.items()
    ]

@app.delete("/api/sessions/{session_id}")
async def delete_session(session_id: str):
    if session_id not in sessions:
        raise HTTPException(status_code=404, detail="Session not found")
    
    try:
        # vectorstore ê°ì²´ ë¨¼ì € ì •ë¦¬
        if 'vectorstore' in sessions[session_id]:
            vectorstore = sessions[session_id]['vectorstore']
            if hasattr(vectorstore, '_client') and vectorstore._client:
                try:
                    vectorstore._client.reset()
                except:
                    pass
            del vectorstore
        
        # ë©”ëª¨ë¦¬ì—ì„œ ì„¸ì…˜ ì œê±°
        del sessions[session_id]
        
        # íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì‚­ì œ (ë¹„ë™ê¸°ì ìœ¼ë¡œ ì²˜ë¦¬)
        session_dir = os.path.join(CHROMA_DB_DIR, session_id)
        if os.path.exists(session_dir):
            import asyncio
            import shutil
            
            async def cleanup_directory():
                max_retries = 3
                for attempt in range(max_retries):
                    try:
                        await asyncio.sleep(0.1)  # ì§§ì€ ëŒ€ê¸°
                        shutil.rmtree(session_dir)
                        break
                    except PermissionError:
                        if attempt < max_retries - 1:
                            await asyncio.sleep(0.5)
                        else:
                            print(f"âš ï¸ ì„¸ì…˜ ë””ë ‰í† ë¦¬ ì‚­ì œ ì§€ì—°ë¨: {session_id}")
            
            # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì‚­ì œ ì‘ì—… ìˆ˜í–‰
            asyncio.create_task(cleanup_directory())
        
        return {"status": "success", "message": "Session deleted"}
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error deleting session: {str(e)}")

def remove_heading(text, heading):
    # "ê°œì„  ì œì•ˆ", "ê°œì„  ì œì•ˆ:", "ê°œì„  ì œì•ˆ " ë“±ìœ¼ë¡œ ì‹œì‘í•˜ë©´ ì œê±°
    for h in [heading, f"{heading}:", f"{heading} "]:
        if text.strip().startswith(h):
            return text.strip()[len(h):].lstrip(": \n\"")
    return text.strip()

def prettify_bullet(text, emoji="â€¢"):
    # '- 'ë¡œ ì‹œì‘í•˜ëŠ” í•­ëª©ì„ ì´ëª¨ì§€ë¡œ ë³€í™˜
    lines = text.split('\n')
    pretty_lines = []
    for line in lines:
        if line.strip().startswith('- '):
            pretty_lines.append(f"{emoji} {line.strip()[2:]}")
        else:
            pretty_lines.append(line.strip())
    return '\n'.join(pretty_lines)

def prettify_feedback(text):
    # ì¥ì /ë¶€ì¡±í•œ ì ì„ ì´ëª¨ì§€ì™€ í•¨ê»˜, ë¦¬ìŠ¤íŠ¸ëŠ” ë“¤ì—¬ì“°ê¸°
    text = text.replace("ì¥ì :", "\nğŸ’¡ ")
    text = text.replace("ë¶€ì¡±í•œ ì :", "\nâš ï¸ ")
    text = text.replace("ê°œì„ í•„ìš”:", "\nğŸ“ ")
    text = text.replace("ê°œì„ ì :", "\nğŸ“ ")
    text = prettify_bullet(text, emoji="ğŸ‘‰")
    return text.strip()

def prettify_evaluation(text):
    return prettify_bullet(text, emoji="âœ…")

def prettify_suggestion(text):
    # ìŒë”°ì˜´í‘œ ì œê±°
    return text.replace('"', '').strip()

def create_chain(vectorstore):
    try:
        print("Creating retriever...")
        retriever = vectorstore.as_retriever(
            search_type="similarity",
            search_kwargs={"k": SEARCH_K}
        )
        print("Retriever created successfully")
        
        template = """
        ë‹¹ì‹ ì€ ê³ ë“±í•™êµ ìƒê¸°ë¶€ íŠ¹ê¸°ì‚¬í•­ ì‘ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

        ì•„ë˜ ì…ë ¥ëœ ë¬¸ì¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ì„¸ ê°€ì§€ í•­ëª©ì„ í‰ê°€í•´ ì£¼ì„¸ìš”:

        â‘  ì í•©ì„± í‰ê°€: í•´ë‹¹ ë¬¸ì¥ì´ ì‘ì„±ìš”ë ¹ì— ì í•©í•œì§€ ê°„ë‹¨íˆ íŒë‹¨í•´ ì£¼ì„¸ìš”.
        â‘¡ ê²€í†  ì˜ê²¬: ë¬¸ì¥ì˜ ì¥ì , ë¶€ì¡±í•œ ì , ê°œì„  í¬ì¸íŠ¸ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
        â‘¢ ê°œì„  ì œì•ˆ: ì‘ì„±ìš”ë ¹ì„ ê³ ë ¤í•˜ì—¬ ë¬¸ì¥ì„ ë” ë‚˜ì€ í˜•íƒœì˜ 500ìë¡œ ìˆ˜ì •í•´ ì£¼ì„¸ìš”.

        ì…ë ¥ ë¬¸ì¥:
        {question}

        â€» ì‘ë‹µì€ ë°˜ë“œì‹œ ìœ„ ìˆœì„œì™€ ë²ˆí˜¸(â‘ â‘¡â‘¢)ë¥¼ í¬í•¨í•˜ì—¬ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.
        """

        print("Creating prompt template...")
        prompt = ChatPromptTemplate.from_template(template)
        print("Prompt template created successfully")
        
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise HTTPException(status_code=500, detail="ANTHROPIC_API_KEY not found")
        
        print("Creating ChatAnthropic model...")
        
        # SSL ì»¨í…ìŠ¤íŠ¸ ìƒì„± ë° ê²€ì¦ ë¹„í™œì„±í™”
        ssl_context = ssl.create_default_context()
        ssl_context.check_hostname = False
        ssl_context.verify_mode = ssl.CERT_NONE
        
        # ì „ì—­ SSL ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        ssl._create_default_https_context = lambda: ssl_context
        
        # í™˜ê²½ ë³€ìˆ˜ë¥¼ í†µí•´ SSL ê²€ì¦ ë¹„í™œì„±í™”
        os.environ['ANTHROPIC_VERIFY_SSL'] = 'false'
        os.environ['REQUESTS_CA_BUNDLE'] = ''
        os.environ['SSL_CERT_FILE'] = ''
        
        model = ChatAnthropic(
            model_name="claude-3-5-sonnet-20241022",
            temperature=0,
            api_key=SecretStr(api_key),
            max_tokens_to_sample=2048,
            timeout=60,
            stop=None
        )
        print("ChatAnthropic model created successfully")
        
        # Anthropic í´ë¼ì´ì–¸íŠ¸ì˜ ë‚´ë¶€ httpx í´ë¼ì´ì–¸íŠ¸ ìˆ˜ì •
        if hasattr(model, '_client') and hasattr(model._client, '_client'):
            # ê¸°ì¡´ í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ë³µì‚¬
            old_client = model._client._client
            # SSL ê²€ì¦ ë¹„í™œì„±í™”ëœ ìƒˆ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            new_client = httpx.Client(
                verify=False,
                timeout=60.0,
                headers=old_client.headers,
                cookies=old_client.cookies,
                auth=old_client.auth,
                follow_redirects=old_client.follow_redirects
            )
            # ë‚´ë¶€ í´ë¼ì´ì–¸íŠ¸ êµì²´
            model._client._client = new_client
            print("Anthropic client SSL verification disabled successfully")
        
        print("Creating chain...")
        chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | model
        )
        print("Chain created successfully")
        
        return chain
    except Exception as e:
        import traceback
        print("=== CREATE CHAIN ERROR ===")
        print(f"Error: {str(e)}")
        traceback.print_exc()
        print("==========================")
        raise HTTPException(status_code=500, detail=f"Chain ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

def cleanup_all_sessions():
    """ì„œë²„ ì¢…ë£Œ ì‹œ ëª¨ë“  ì„¸ì…˜ ì •ë¦¬"""
    print("ğŸ§¹ ì„œë²„ ì¢…ë£Œ ì¤‘ - ëª¨ë“  ì„¸ì…˜ ì •ë¦¬...")
    for session_id in list(sessions.keys()):
        try:
            if 'vectorstore' in sessions[session_id]:
                vectorstore = sessions[session_id]['vectorstore']
                if hasattr(vectorstore, '_client') and vectorstore._client:
                    vectorstore._client.reset()
        except:
            pass
    sessions.clear()

# ì„œë²„ ì¢…ë£Œ ì‹œ ì •ë¦¬ í•¨ìˆ˜ ë“±ë¡
atexit.register(cleanup_all_sessions)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 